{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, (128, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, (1, 3), padding=(0,1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, (1, 3), padding=(0,1)),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.skip1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, (1, 1)),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.sum1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, (1, 3), stride=(1,2), padding=(0,1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, (1, 3), padding=(0,1)),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.skip2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, (1, 1), stride=(1,2)),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.sum2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, (1, 3), stride=(1,2), padding=(0,1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, (1, 3), padding=(0,1)),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        self.skip3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, (1, 1), stride=(1,2)),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        self.sum3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, (1, 3), stride=(1,2), padding=(0,1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, (1, 3), padding=(0,1)),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "\n",
    "        self.skip4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, (1, 1), stride=(1,2)),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "\n",
    "        self.sum4 = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.global_pooling = nn.AvgPool2d((1, 156))\n",
    "\n",
    "        self.dense = nn.Linear(512, 8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv0(x)\n",
    "        x1 = self.sum1(self.conv1(x0) + self.skip1(x0))\n",
    "        x2 = self.sum2(self.conv2(x1) + self.skip2(x1))\n",
    "        x3 = self.sum3(self.conv3(x2) + self.skip3(x2))\n",
    "        x4 = self.sum4(self.conv4(x3) + self.skip4(x3))\n",
    "        x5 = self.global_pooling(x4)\n",
    "\n",
    "        x6 = x5.view(x5.shape[0], -1)\n",
    "        x7 = self.dense(x6)\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pickles/models/custom_resnet_model_55.p', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MyModel' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0ce7c4921d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MyModel' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(128, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 2))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (8): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (9): Sequential(\n",
       "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (10): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(1, 3), stride=(1, 2), padding=(0, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (11): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 2))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (12): Sequential(\n",
       "    (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (13): AvgPool2d(kernel_size=(1, 156), stride=(1, 156), padding=0)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "newmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pickles/main_dict.pickle', 'rb') as f:\n",
    "    main_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/spectrograms/train/train/'\n",
    "val_dir = '../data/spectrograms/train/val/'\n",
    "test_dir = '../data/spectrograms/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.0069], [0.0033])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolderWithPaths(train_dir, one_transform)\n",
    "val_dataset = ImageFolderWithPaths(val_dir, one_transform)\n",
    "\n",
    "batch_size = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=8\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n",
    ")\n",
    "\n",
    "class_names = train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageFolderWithPaths(test_dir, one_transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs, x, path in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = torch.nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(model, test_dataloader)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "preds = [class_names[x]for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_class = list()\n",
    "for x in test_dataset:\n",
    "    path = x[2]\n",
    "    name = path.split('/')[5].strip('.png')\n",
    "    genre = main_dict[int(name)]['genre']\n",
    "    true_class.append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5979381443298969"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.array(true_class) == np.array(preds)).sum() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6068307662518424"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(true_class, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = []\n",
    "\n",
    "    for inputs, x, path in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        outputs = model(inputs).cpu()\n",
    "        outputs = outputs.view(outputs.shape[0], -1)\n",
    "        logits.append(outputs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3533e+00, 7.6774e-02, 2.3122e-01, 2.0391e-01, 1.1807e-02, 2.2045e-01,\n",
       "        1.9637e+00, 6.6312e-01, 2.4221e-01, 3.9555e-01, 1.5470e+00, 6.9158e-02,\n",
       "        1.9168e-01, 2.7678e-01, 1.3720e+00, 4.5506e-01, 6.8681e-01, 7.7914e-01,\n",
       "        4.2933e-01, 5.5080e-01, 5.2408e-02, 1.1993e-01, 1.8741e-01, 1.1234e+00,\n",
       "        1.2587e-01, 2.1183e-01, 5.3860e-01, 8.9890e-01, 1.5663e-01, 9.1550e-01,\n",
       "        3.2602e-01, 1.2392e+00, 2.5497e-01, 2.3628e-01, 4.3947e-02, 2.9166e-01,\n",
       "        3.4358e-01, 3.7946e-01, 1.4876e+00, 1.2251e-01, 2.2617e-01, 7.0939e-01,\n",
       "        1.0598e+00, 9.8894e-01, 1.2447e+00, 1.3121e-02, 7.0357e-01, 4.5590e-01,\n",
       "        2.0090e-01, 3.1472e-01, 8.2325e-02, 3.5095e-01, 2.8994e-02, 1.2500e-01,\n",
       "        1.0103e+00, 8.8144e-02, 1.3596e-01, 7.3645e-02, 1.4827e-01, 3.2517e-01,\n",
       "        1.6554e-01, 3.4745e-01, 3.7581e-01, 3.2449e-01, 8.9321e-03, 1.8314e-01,\n",
       "        4.0948e-01, 2.4520e-01, 4.7574e-01, 1.0919e+00, 1.4649e+00, 2.3113e-02,\n",
       "        1.0937e-01, 4.8258e-01, 1.0302e-01, 6.5178e-02, 1.2674e+00, 1.4066e+00,\n",
       "        8.6347e-02, 5.8885e-01, 1.0623e+00, 5.2773e-01, 3.9482e-01, 9.0930e-02,\n",
       "        4.7913e-01, 5.2260e-01, 1.7286e-01, 1.6361e+00, 1.3521e-01, 4.7671e-02,\n",
       "        6.5113e-02, 1.2300e+00, 1.1912e-01, 5.9219e-01, 1.9689e-01, 3.1978e-01,\n",
       "        2.4309e-01, 1.0272e+00, 1.6751e-01, 6.7786e-01, 2.8522e-01, 1.4868e+00,\n",
       "        1.8851e-01, 2.2592e-01, 3.8024e-01, 6.0307e-02, 3.6012e-02, 1.6455e-02,\n",
       "        1.1945e+00, 2.3488e-01, 1.6327e+00, 1.1073e-01, 7.2347e-02, 7.1904e-02,\n",
       "        1.3339e+00, 5.3971e-01, 7.6496e-01, 3.0243e-01, 1.2040e-01, 5.3170e-01,\n",
       "        5.8163e-02, 9.6803e-02, 1.1638e-01, 3.4173e-01, 1.2445e-01, 7.2560e-02,\n",
       "        1.0562e+00, 1.7578e-01, 1.8759e-01, 1.9249e-02, 5.0609e-02, 8.5871e-01,\n",
       "        2.7276e-01, 7.0001e-02, 1.2868e+00, 1.0093e+00, 6.2589e-01, 4.6607e-01,\n",
       "        1.1578e+00, 6.6114e-02, 7.8587e-01, 9.5830e-01, 4.6850e-01, 2.6380e-03,\n",
       "        1.4872e-01, 1.4415e-01, 4.3050e-01, 1.6751e+00, 4.4582e-02, 5.4231e-01,\n",
       "        1.3394e-02, 6.0308e-01, 4.2870e-01, 8.0596e-02, 6.3485e-01, 2.1120e-01,\n",
       "        1.5754e-01, 1.0597e+00, 3.0132e-01, 4.5698e-01, 4.7464e-02, 2.7682e-01,\n",
       "        3.0397e-01, 3.5315e-01, 1.1469e+00, 3.6173e-02, 1.8164e-01, 9.2187e-01,\n",
       "        1.2594e-01, 3.2559e-01, 1.1352e-01, 4.6223e-02, 5.1515e-02, 1.0058e-01,\n",
       "        3.5172e-02, 6.8839e-02, 1.3981e-01, 7.4520e-01, 2.9349e-01, 7.4768e-01,\n",
       "        5.0876e-01, 2.0401e-01, 6.7898e-01, 1.2635e+00, 7.7131e-01, 1.7876e-01,\n",
       "        3.9937e-01, 2.9513e-01, 1.6193e+00, 1.2532e+00, 4.4348e-01, 1.1546e+00,\n",
       "        5.7797e-03, 2.5760e-01, 1.0315e-01, 1.1237e+00, 1.4989e-01, 2.4707e-01,\n",
       "        1.3923e+00, 3.8236e-01, 1.6223e-01, 8.9356e-01, 2.9913e-01, 2.7462e-01,\n",
       "        4.7306e-02, 7.1849e-01, 1.1720e-01, 1.8188e-01, 1.2540e+00, 3.7293e-01,\n",
       "        8.8594e-02, 5.1376e-02, 2.5277e-01, 4.1899e-01, 5.4810e-02, 2.4230e-01,\n",
       "        6.7982e-02, 2.4242e-01, 5.6859e-02, 1.5948e-01, 2.2660e-01, 7.3838e-02,\n",
       "        1.1421e-01, 3.1073e-01, 1.3804e-01, 1.3606e+00, 9.5652e-02, 6.4988e-01,\n",
       "        7.0015e-01, 1.2343e+00, 2.3378e-01, 1.0508e-01, 6.6597e-01, 1.8400e-02,\n",
       "        1.1415e-01, 9.2178e-02, 1.9994e+00, 1.0784e+00, 1.3626e+00, 3.4732e-01,\n",
       "        5.0533e-01, 1.1805e-01, 5.1456e-02, 8.7439e-01, 1.3495e-02, 6.1704e-01,\n",
       "        7.4786e-03, 1.0533e+00, 1.1639e+00, 7.2344e-01, 7.4982e-02, 2.2414e-01,\n",
       "        1.1446e+00, 5.6778e-01, 1.9868e-01, 3.0289e-02, 9.6497e-02, 4.0132e-01,\n",
       "        1.6369e-01, 7.7815e-02, 1.5079e+00, 2.1182e-01, 5.7454e-02, 1.0028e+00,\n",
       "        1.4157e-01, 1.2416e-01, 8.5772e-02, 1.9593e-01, 1.3513e+00, 6.2875e-02,\n",
       "        8.5039e-01, 7.6083e-01, 9.7034e-02, 2.1833e-01, 2.6005e-01, 5.4608e-01,\n",
       "        1.0984e-01, 1.1362e+00, 4.0053e-01, 8.1455e-02, 1.5483e+00, 3.0227e-01,\n",
       "        1.0815e+00, 1.4994e+00, 1.6718e-01, 1.3224e-01, 3.0286e-02, 1.1068e+00,\n",
       "        1.1292e+00, 2.1026e-01, 4.1921e-01, 3.2434e-01, 5.7364e-02, 1.8281e-01,\n",
       "        1.7149e-01, 4.4118e-01, 3.3646e-01, 4.8050e-01, 1.4252e-02, 2.8632e-01,\n",
       "        1.1873e+00, 6.9792e-01, 3.6934e-01, 2.0163e-01, 2.4936e-01, 3.5463e-01,\n",
       "        8.3344e-01, 1.5480e+00, 1.1523e+00, 1.5920e+00, 6.0942e-01, 9.4546e-01,\n",
       "        2.3604e-02, 1.5379e+00, 5.9367e-02, 1.0805e+00, 9.6895e-04, 1.3903e-01,\n",
       "        4.4548e-01, 1.7333e-02, 1.1936e+00, 1.1082e+00, 1.2456e+00, 7.4543e-02,\n",
       "        3.7814e-02, 9.2887e-02, 2.6390e-01, 5.9137e-02, 1.8829e-01, 1.4926e-01,\n",
       "        9.0235e-02, 1.7248e-01, 8.7337e-01, 1.3687e+00, 1.3203e+00, 1.9867e-01,\n",
       "        7.1297e-02, 1.1008e+00, 6.4400e-01, 1.5398e+00, 3.6990e-01, 1.6737e+00,\n",
       "        6.7415e-02, 7.7697e-01, 1.4180e+00, 2.0773e-01, 2.6039e-01, 1.8327e-01,\n",
       "        1.5962e-01, 1.0792e+00, 8.4994e-01, 9.8200e-02, 6.7670e-02, 1.1757e-01,\n",
       "        8.7283e-02, 6.9127e-02, 4.4750e-02, 6.9808e-01, 2.8989e-01, 1.8565e-01,\n",
       "        1.3934e+00, 5.3444e-02, 9.4800e-02, 2.0190e-01, 2.3034e-01, 2.1825e-01,\n",
       "        1.4307e-01, 1.7683e-01, 3.1891e-02, 5.7652e-02, 1.9214e-01, 1.9529e-01,\n",
       "        3.5564e-01, 1.0747e-01, 6.6488e-01, 1.5186e+00, 5.0313e-02, 1.3248e-01,\n",
       "        1.4581e+00, 2.9690e-01, 1.7106e+00, 3.1775e-01, 5.3580e-01, 4.7284e-01,\n",
       "        1.8548e-01, 6.5019e-02, 3.7020e-02, 2.7363e-01, 1.3248e-01, 6.2443e-01,\n",
       "        1.5190e-01, 1.5406e-01, 1.0879e-01, 8.8151e-01, 2.5894e-01, 8.5293e-01,\n",
       "        2.0923e-01, 3.7708e-02, 9.1432e-02, 6.3568e-02, 2.7199e-01, 3.0442e-01,\n",
       "        3.2789e-01, 5.8641e-03, 9.8805e-02, 8.8552e-01, 4.7264e-01, 1.2504e+00,\n",
       "        3.4264e-01, 3.2515e-01, 1.3150e-01, 6.8803e-01, 5.0143e-02, 1.0857e-01,\n",
       "        1.4371e+00, 5.8042e-01, 1.3437e+00, 1.5180e-01, 8.7304e-02, 2.0898e-01,\n",
       "        1.8362e-01, 3.5245e-01, 2.7388e-01, 4.9924e-01, 1.5474e-01, 2.9770e-01,\n",
       "        9.9623e-01, 1.3012e-01, 2.2936e-01, 1.1012e+00, 6.6733e-02, 4.1876e-01,\n",
       "        4.7564e-01, 1.0755e-01, 6.3887e-01, 6.8211e-01, 1.9689e-01, 5.3977e-02,\n",
       "        1.7834e-01, 1.1695e-01, 4.6234e-02, 4.9712e-02, 1.5463e-01, 1.6665e+00,\n",
       "        2.8851e-01, 7.2424e-02, 1.8150e-02, 2.9931e-02, 9.2561e-01, 2.9928e-02,\n",
       "        1.7625e-01, 8.0434e-01, 1.5839e+00, 3.6712e-01, 2.2088e-01, 2.0962e-01,\n",
       "        1.1184e+00, 4.1847e-02, 1.1946e+00, 1.6801e+00, 3.1356e-01, 4.4226e-01,\n",
       "        1.2298e-03, 1.7354e-01, 4.4983e-01, 3.0997e-01, 7.9765e-01, 5.0075e-01,\n",
       "        9.1858e-01, 6.0304e-01, 1.9330e-01, 2.9173e-02, 4.1242e-01, 2.8948e-03,\n",
       "        4.8984e-01, 4.1313e-01, 7.3460e-02, 1.9672e-01, 1.7432e+00, 2.9407e-02,\n",
       "        1.1721e-01, 1.0788e-01, 1.4964e-01, 1.5383e+00, 1.6707e-01, 1.4802e+00,\n",
       "        2.6854e-01, 8.4982e-01, 3.9174e-02, 3.1303e-01, 9.1489e-01, 1.9448e-02,\n",
       "        5.6227e-01, 1.0056e+00, 2.4335e-01, 1.5959e-01, 6.3434e-01, 1.0696e-01,\n",
       "        1.1575e-02, 2.1129e+00, 2.9585e-01, 5.5903e-02, 1.4638e-01, 2.2814e-02,\n",
       "        1.1877e-01, 4.7170e-02, 3.7215e-02, 1.2306e+00, 6.8908e-02, 1.8414e-01,\n",
       "        1.8190e-01, 9.0591e-01])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'MyModel' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-cf4018352c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MyModel' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "flat_model = model[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = []\n",
    "    path_list = list()\n",
    "    for inputs, x, path in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        outputs = model(inputs).cpu()\n",
    "        outputs = outputs.view(outputs.shape[0], -1)\n",
    "        logits.append(outputs)\n",
    "        path_list.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     logits = []\n",
    "#     path_list = list()\n",
    "    for inputs, x, path in val_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        outputs = model(inputs).cpu()\n",
    "        outputs = outputs.view(outputs.shape[0], -1)\n",
    "        logits.append(outputs)\n",
    "        path_list.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#     logits = []\n",
    "#     path_list = list()\n",
    "    for inputs, x, path in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        outputs = model(inputs).cpu()\n",
    "        outputs = outputs.view(outputs.shape[0], -1)\n",
    "        logits.append(outputs)\n",
    "        path_list.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/spectrograms/test/unknown/000190.png'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2660e-01, 9.4770e-01, 4.0977e-02, 1.1206e-01, 7.0712e-01, 1.1625e+00,\n",
       "        8.0353e-01, 6.7246e-02, 1.1167e-01, 7.2490e-01, 9.4193e-01, 3.1589e+00,\n",
       "        1.7728e+00, 6.7006e-03, 7.4222e-02, 8.1362e-02, 1.0331e+00, 6.4309e-01,\n",
       "        1.5609e-01, 1.0863e-01, 3.0952e-01, 1.1050e-01, 2.0864e+00, 2.7958e+00,\n",
       "        8.9385e-01, 4.9957e-01, 2.3699e+00, 8.8438e-01, 1.4698e+00, 4.6339e-03,\n",
       "        4.2226e-02, 7.0422e-02, 1.1675e+00, 3.7796e-01, 1.4244e+00, 1.1887e+00,\n",
       "        9.1567e-01, 6.1085e-01, 5.3914e-01, 3.3007e-02, 1.0496e+00, 0.0000e+00,\n",
       "        1.7774e+00, 1.4602e+00, 9.1238e-01, 2.5805e-01, 1.1028e+00, 1.5210e+00,\n",
       "        2.2665e+00, 6.4750e-01, 2.1433e-01, 8.4579e-01, 9.1978e-01, 1.3228e+00,\n",
       "        1.3943e+00, 5.5475e-01, 3.1136e+00, 2.4588e+00, 5.6724e-01, 5.0780e-01,\n",
       "        1.3105e-02, 6.7418e-01, 3.2605e-01, 1.6718e+00, 3.6741e-03, 2.5260e-01,\n",
       "        4.9692e-01, 5.5080e-01, 3.3561e-01, 1.2929e+00, 1.3430e-01, 6.5861e-02,\n",
       "        3.0020e+00, 1.9675e+00, 5.1272e-01, 1.7287e-01, 2.4030e-01, 3.6033e-01,\n",
       "        3.7455e-01, 4.0795e+00, 1.5439e+00, 1.8617e+00, 2.8538e-01, 3.5068e-01,\n",
       "        6.7507e-01, 3.2062e-02, 5.2502e-01, 8.0413e-01, 1.7256e+00, 3.2750e-01,\n",
       "        3.2452e-01, 1.2776e+00, 2.0435e-02, 6.2409e-01, 7.5334e-02, 8.0563e-01,\n",
       "        1.9893e-01, 1.6238e-02, 4.1495e-01, 2.4810e+00, 1.2220e+00, 1.5929e+00,\n",
       "        4.9405e-01, 2.2240e+00, 1.2020e+00, 9.0156e-01, 1.3145e-01, 5.6099e-01,\n",
       "        1.2947e+00, 1.0577e+00, 1.1630e-01, 1.8022e-02, 2.3063e+00, 2.4244e+00,\n",
       "        8.0338e-02, 1.4302e+00, 9.6643e-01, 1.5025e+00, 1.6548e+00, 7.9930e-01,\n",
       "        1.4291e+00, 8.1205e-01, 1.8634e+00, 2.3683e+00, 3.2792e-01, 2.0984e+00,\n",
       "        5.7783e-03, 1.8097e+00, 3.4152e+00, 1.7976e+00, 2.7152e-01, 2.9611e-01,\n",
       "        1.5805e+00, 1.1328e+00, 2.3651e-01, 2.1568e-01, 3.2827e+00, 7.5686e-01,\n",
       "        3.2226e-01, 9.1006e-01, 7.3971e-01, 7.2072e-01, 7.4331e-01, 2.5855e-02,\n",
       "        3.5576e+00, 2.7833e+00, 4.1077e-01, 1.7121e-01, 6.7049e-01, 6.8274e-02,\n",
       "        4.5828e-01, 1.9641e-01, 9.1716e-02, 7.2683e-01, 5.3625e-02, 2.0344e-01,\n",
       "        2.3829e+00, 1.2731e+00, 4.8609e-01, 4.1417e-01, 5.6762e-01, 1.9478e-01,\n",
       "        4.2990e-01, 2.6705e-02, 1.0690e-01, 1.0070e+00, 4.7461e-01, 3.4651e-01,\n",
       "        1.1494e+00, 4.8907e-01, 4.1913e+00, 2.2516e+00, 2.6361e-01, 8.4972e-01,\n",
       "        1.0637e+00, 1.1515e+00, 3.2536e-01, 1.0113e+00, 2.5190e+00, 4.0538e-01,\n",
       "        2.8749e+00, 1.7985e+00, 7.3851e-02, 1.1645e-01, 4.5056e-01, 3.9724e-01,\n",
       "        1.8540e-02, 3.3338e+00, 2.1356e-01, 1.2925e+00, 3.7696e+00, 1.6394e+00,\n",
       "        7.9386e-01, 2.4330e-01, 1.2024e+00, 6.0507e-01, 8.2217e-01, 3.7771e+00,\n",
       "        9.7850e-01, 2.2263e-01, 3.8944e-01, 3.6625e+00, 2.0384e+00, 1.5167e+00,\n",
       "        3.3657e-01, 5.1635e-01, 6.7857e-01, 3.2991e-01, 5.7638e-01, 2.2367e-02,\n",
       "        1.7847e-02, 1.3761e-01, 4.9942e-01, 2.3354e+00, 1.8211e-01, 2.1211e-01,\n",
       "        4.4011e-02, 3.1402e+00, 5.1420e-01, 6.4377e-02, 3.1082e-01, 2.3308e+00,\n",
       "        1.1079e+00, 8.2413e-01, 8.5387e-02, 2.0771e+00, 7.7609e-02, 2.7484e-01,\n",
       "        1.5336e+00, 6.0407e-02, 3.4209e-01, 1.1728e-01, 2.4631e+00, 1.1951e+00,\n",
       "        2.7097e+00, 1.6490e+00, 6.3072e-01, 9.0874e-02, 6.1125e-01, 4.0515e-01,\n",
       "        1.0679e-02, 9.5179e-02, 9.4712e-01, 2.7681e+00, 2.4359e-01, 2.0258e+00,\n",
       "        6.5821e-01, 3.5558e-01, 1.3185e+00, 1.7764e+00, 1.0653e+00, 2.9523e-01,\n",
       "        3.0473e-02, 1.5209e+00, 1.8781e-01, 1.4047e+00, 1.8634e+00, 1.9168e+00,\n",
       "        6.3362e-01, 2.5537e-02, 6.2611e-02, 1.1918e+00, 5.2006e-02, 1.0424e-01,\n",
       "        1.2414e-01, 2.5131e+00, 7.1843e-03, 2.3555e-01, 3.3595e-01, 9.6797e-01,\n",
       "        3.6524e-01, 3.3031e-02, 2.5976e+00, 7.0219e-02, 3.7410e-01, 1.3040e+00,\n",
       "        1.7034e+00, 4.2026e-02, 1.7414e+00, 1.2364e+00, 9.0017e-02, 1.3274e+00,\n",
       "        3.0593e-01, 1.4992e+00, 2.6962e-01, 2.5260e-01, 1.5427e+00, 8.1883e-01,\n",
       "        2.8968e-01, 5.9271e-01, 7.3577e-01, 1.7165e-01, 6.7906e-01, 2.1343e+00,\n",
       "        9.2470e-03, 4.1141e-01, 3.1363e+00, 1.3289e-02, 2.4196e-01, 8.9577e-01,\n",
       "        1.6286e+00, 2.3091e+00, 1.9501e+00, 1.6708e+00, 1.0862e+00, 5.7653e-02,\n",
       "        4.3263e-02, 2.1175e-01, 1.2401e-01, 9.5505e-01, 1.0798e-01, 1.4127e+00,\n",
       "        4.9876e-01, 4.6740e-01, 2.2555e+00, 7.7636e-02, 2.7853e-02, 2.5638e+00,\n",
       "        7.4779e-01, 1.5716e-01, 5.3501e-01, 1.8669e-01, 7.7304e-02, 2.5177e-01,\n",
       "        3.1067e+00, 2.1895e-01, 3.3357e-01, 2.6896e+00, 1.0338e+00, 8.0304e-01,\n",
       "        1.0635e+00, 2.4213e+00, 2.0607e+00, 1.9944e-01, 1.6608e-01, 1.5442e-01,\n",
       "        1.5849e+00, 2.8474e-01, 3.9551e-01, 1.0811e-02, 2.1207e+00, 3.0014e-01,\n",
       "        2.7053e+00, 3.0756e-01, 2.6768e+00, 3.4613e+00, 4.6176e-01, 1.5378e+00,\n",
       "        1.1521e+00, 6.1038e-01, 2.6091e-01, 2.1271e-01, 1.3461e-01, 2.9995e-01,\n",
       "        2.5047e+00, 3.2327e-01, 1.2019e-01, 7.5309e-02, 1.3677e+00, 1.7185e+00,\n",
       "        2.0660e+00, 1.8551e-01, 6.5620e-02, 2.7019e-02, 4.4564e+00, 1.4193e+00,\n",
       "        4.4559e-01, 1.4454e-01, 1.0311e-02, 2.2641e+00, 1.1580e-01, 1.2620e+00,\n",
       "        8.5961e-02, 1.4409e+00, 1.5892e+00, 4.5780e-01, 5.3525e-01, 1.4162e-01,\n",
       "        7.7785e-01, 1.4200e+00, 6.7433e-02, 2.3633e+00, 1.1999e-01, 4.4010e-01,\n",
       "        1.3608e+00, 1.0367e+00, 1.3416e+00, 2.6647e-02, 2.4783e+00, 2.3880e+00,\n",
       "        2.3205e+00, 3.8377e-01, 2.8208e+00, 9.6400e-02, 9.0994e-02, 3.1346e-01,\n",
       "        8.3843e-01, 6.7224e-01, 8.1236e-01, 2.4470e+00, 1.7599e+00, 7.8284e-02,\n",
       "        2.5636e+00, 1.1912e-01, 1.4324e-01, 1.8051e+00, 1.4659e+00, 9.1464e-02,\n",
       "        4.8143e-02, 4.2235e-01, 8.1536e-01, 3.1522e+00, 1.8342e+00, 6.8396e-03,\n",
       "        6.2443e-01, 3.7595e-01, 1.2481e-01, 4.8051e+00, 1.9384e-01, 2.7850e-01,\n",
       "        1.2185e+00, 2.9702e+00, 3.3254e-01, 4.0770e-01, 2.0255e+00, 1.0970e-01,\n",
       "        8.1053e-01, 1.8603e-01, 3.4251e-02, 3.0024e+00, 4.0772e-01, 1.2954e+00,\n",
       "        3.4898e-02, 1.3680e-01, 1.3287e+00, 2.1705e-01, 1.8994e+00, 1.4420e+00,\n",
       "        2.5700e+00, 1.5798e-01, 2.3658e+00, 2.6899e+00, 3.4160e+00, 2.8073e+00,\n",
       "        2.0390e-01, 2.1172e-01, 6.2857e-01, 1.2966e+00, 1.2575e+00, 5.4514e-01,\n",
       "        4.9591e-02, 1.1177e+00, 7.7439e-01, 1.3782e-01, 2.9004e-02, 1.7375e+00,\n",
       "        2.9908e-02, 4.6440e-01, 7.0170e-01, 9.3899e-01, 3.0922e-01, 1.0843e-01,\n",
       "        1.6521e+00, 4.5348e-01, 1.3954e+00, 4.6409e-01, 4.4304e-01, 8.9522e-02,\n",
       "        6.3792e-01, 1.4847e+00, 4.1910e-03, 7.8348e-01, 1.9726e+00, 7.7659e-02,\n",
       "        8.4446e-02, 1.1946e-01, 2.6970e+00, 3.7714e-01, 7.5285e-01, 2.7813e+00,\n",
       "        2.8033e-02, 2.9686e+00, 4.5477e-01, 2.5212e-02, 3.0577e-01, 9.0169e-01,\n",
       "        1.7400e-01, 4.4362e-01, 8.6072e-02, 1.6390e-01, 6.7695e-01, 4.9022e-02,\n",
       "        1.4358e-01, 1.2628e-01, 4.3060e-01, 6.8017e-01, 1.6154e+00, 1.0520e+00,\n",
       "        9.0169e-01, 1.5075e-01, 1.3647e-01, 9.1344e-02, 5.5059e-01, 1.3641e-01,\n",
       "        2.7790e+00, 3.9962e-02, 1.9097e-01, 3.2118e+00, 3.4027e-01, 5.5292e-01,\n",
       "        1.5535e-01, 4.5735e-01])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list = list()\n",
    "for i in logits:\n",
    "    for tens in i:\n",
    "        emb_list.append(tens.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = pd.DataFrame(emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_list = list()\n",
    "for i in path_list:\n",
    "    for path in i:\n",
    "        name = path.split('/')[-1]\n",
    "        ids = name.split('.')[0]\n",
    "        track_list.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings['id'] = track_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.426598</td>\n",
       "      <td>0.947701</td>\n",
       "      <td>0.040977</td>\n",
       "      <td>0.112058</td>\n",
       "      <td>0.707118</td>\n",
       "      <td>1.162531</td>\n",
       "      <td>0.803530</td>\n",
       "      <td>0.067246</td>\n",
       "      <td>0.111666</td>\n",
       "      <td>0.724896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136411</td>\n",
       "      <td>2.779022</td>\n",
       "      <td>0.039962</td>\n",
       "      <td>0.190971</td>\n",
       "      <td>3.211765</td>\n",
       "      <td>0.340266</td>\n",
       "      <td>0.552923</td>\n",
       "      <td>0.155349</td>\n",
       "      <td>0.457349</td>\n",
       "      <td>000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.958111</td>\n",
       "      <td>0.308117</td>\n",
       "      <td>0.088165</td>\n",
       "      <td>0.071058</td>\n",
       "      <td>0.431378</td>\n",
       "      <td>0.324224</td>\n",
       "      <td>0.839473</td>\n",
       "      <td>0.133348</td>\n",
       "      <td>0.122043</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297063</td>\n",
       "      <td>1.048719</td>\n",
       "      <td>0.030102</td>\n",
       "      <td>0.299465</td>\n",
       "      <td>2.430354</td>\n",
       "      <td>0.070081</td>\n",
       "      <td>0.408074</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>0.319569</td>\n",
       "      <td>000194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.726996</td>\n",
       "      <td>0.837075</td>\n",
       "      <td>0.141356</td>\n",
       "      <td>0.988398</td>\n",
       "      <td>0.062930</td>\n",
       "      <td>0.058893</td>\n",
       "      <td>0.221997</td>\n",
       "      <td>0.214702</td>\n",
       "      <td>0.170721</td>\n",
       "      <td>0.858936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247367</td>\n",
       "      <td>0.751735</td>\n",
       "      <td>0.037458</td>\n",
       "      <td>0.535053</td>\n",
       "      <td>1.876652</td>\n",
       "      <td>0.072166</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>0.374152</td>\n",
       "      <td>0.979964</td>\n",
       "      <td>000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.353337</td>\n",
       "      <td>0.076774</td>\n",
       "      <td>0.231223</td>\n",
       "      <td>0.203908</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>0.220445</td>\n",
       "      <td>1.963683</td>\n",
       "      <td>0.663118</td>\n",
       "      <td>0.242210</td>\n",
       "      <td>0.395554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022814</td>\n",
       "      <td>0.118767</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.037215</td>\n",
       "      <td>1.230592</td>\n",
       "      <td>0.068908</td>\n",
       "      <td>0.184140</td>\n",
       "      <td>0.181901</td>\n",
       "      <td>0.905914</td>\n",
       "      <td>001040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.857120</td>\n",
       "      <td>0.489802</td>\n",
       "      <td>0.093675</td>\n",
       "      <td>0.177646</td>\n",
       "      <td>0.443665</td>\n",
       "      <td>0.318869</td>\n",
       "      <td>0.174736</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.147258</td>\n",
       "      <td>0.775616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272839</td>\n",
       "      <td>2.666928</td>\n",
       "      <td>0.069785</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>2.577690</td>\n",
       "      <td>0.188910</td>\n",
       "      <td>0.500382</td>\n",
       "      <td>0.123293</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>001686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>1.123316</td>\n",
       "      <td>0.257146</td>\n",
       "      <td>0.041416</td>\n",
       "      <td>0.331017</td>\n",
       "      <td>0.009278</td>\n",
       "      <td>0.060962</td>\n",
       "      <td>1.558685</td>\n",
       "      <td>0.512249</td>\n",
       "      <td>0.190467</td>\n",
       "      <td>0.637470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226283</td>\n",
       "      <td>0.032925</td>\n",
       "      <td>0.083238</td>\n",
       "      <td>0.083157</td>\n",
       "      <td>1.436705</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.090603</td>\n",
       "      <td>0.087154</td>\n",
       "      <td>0.878640</td>\n",
       "      <td>149416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>0.661392</td>\n",
       "      <td>0.269433</td>\n",
       "      <td>0.034565</td>\n",
       "      <td>0.227549</td>\n",
       "      <td>0.034320</td>\n",
       "      <td>0.272772</td>\n",
       "      <td>1.513012</td>\n",
       "      <td>0.582256</td>\n",
       "      <td>0.098210</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080972</td>\n",
       "      <td>0.225576</td>\n",
       "      <td>0.043214</td>\n",
       "      <td>0.089456</td>\n",
       "      <td>1.589537</td>\n",
       "      <td>0.057539</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.031669</td>\n",
       "      <td>0.655243</td>\n",
       "      <td>149417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>1.626190</td>\n",
       "      <td>0.082549</td>\n",
       "      <td>1.335705</td>\n",
       "      <td>0.973848</td>\n",
       "      <td>0.268110</td>\n",
       "      <td>0.928231</td>\n",
       "      <td>1.823195</td>\n",
       "      <td>1.195325</td>\n",
       "      <td>1.626279</td>\n",
       "      <td>0.178328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036719</td>\n",
       "      <td>0.128401</td>\n",
       "      <td>0.037347</td>\n",
       "      <td>0.027686</td>\n",
       "      <td>0.201245</td>\n",
       "      <td>0.149208</td>\n",
       "      <td>0.136870</td>\n",
       "      <td>1.800367</td>\n",
       "      <td>1.064121</td>\n",
       "      <td>149452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>1.385681</td>\n",
       "      <td>0.296244</td>\n",
       "      <td>0.075858</td>\n",
       "      <td>0.372868</td>\n",
       "      <td>0.048158</td>\n",
       "      <td>0.195493</td>\n",
       "      <td>0.737243</td>\n",
       "      <td>0.087842</td>\n",
       "      <td>0.119013</td>\n",
       "      <td>0.828757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672356</td>\n",
       "      <td>0.633477</td>\n",
       "      <td>0.091810</td>\n",
       "      <td>0.047381</td>\n",
       "      <td>2.137580</td>\n",
       "      <td>0.119381</td>\n",
       "      <td>0.112560</td>\n",
       "      <td>0.056708</td>\n",
       "      <td>1.155992</td>\n",
       "      <td>149488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>0.207741</td>\n",
       "      <td>0.978599</td>\n",
       "      <td>0.469821</td>\n",
       "      <td>0.191803</td>\n",
       "      <td>0.910989</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>1.368598</td>\n",
       "      <td>0.058850</td>\n",
       "      <td>0.195802</td>\n",
       "      <td>0.331647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055077</td>\n",
       "      <td>0.407148</td>\n",
       "      <td>0.098213</td>\n",
       "      <td>1.139604</td>\n",
       "      <td>0.302552</td>\n",
       "      <td>0.024557</td>\n",
       "      <td>0.177658</td>\n",
       "      <td>0.374513</td>\n",
       "      <td>0.113956</td>\n",
       "      <td>149523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7994 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.426598  0.947701  0.040977  0.112058  0.707118  1.162531  0.803530   \n",
       "1     0.958111  0.308117  0.088165  0.071058  0.431378  0.324224  0.839473   \n",
       "2     2.726996  0.837075  0.141356  0.988398  0.062930  0.058893  0.221997   \n",
       "3     1.353337  0.076774  0.231223  0.203908  0.011807  0.220445  1.963683   \n",
       "4     1.857120  0.489802  0.093675  0.177646  0.443665  0.318869  0.174736   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7989  1.123316  0.257146  0.041416  0.331017  0.009278  0.060962  1.558685   \n",
       "7990  0.661392  0.269433  0.034565  0.227549  0.034320  0.272772  1.513012   \n",
       "7991  1.626190  0.082549  1.335705  0.973848  0.268110  0.928231  1.823195   \n",
       "7992  1.385681  0.296244  0.075858  0.372868  0.048158  0.195493  0.737243   \n",
       "7993  0.207741  0.978599  0.469821  0.191803  0.910989  0.031496  1.368598   \n",
       "\n",
       "             7         8         9  ...       503       504       505  \\\n",
       "0     0.067246  0.111666  0.724896  ...  0.136411  2.779022  0.039962   \n",
       "1     0.133348  0.122043  0.999581  ...  0.297063  1.048719  0.030102   \n",
       "2     0.214702  0.170721  0.858936  ...  0.247367  0.751735  0.037458   \n",
       "3     0.663118  0.242210  0.395554  ...  0.022814  0.118767  0.047170   \n",
       "4     0.002716  0.147258  0.775616  ...  0.272839  2.666928  0.069785   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7989  0.512249  0.190467  0.637470  ...  0.226283  0.032925  0.083238   \n",
       "7990  0.582256  0.098210  0.393443  ...  0.080972  0.225576  0.043214   \n",
       "7991  1.195325  1.626279  0.178328  ...  0.036719  0.128401  0.037347   \n",
       "7992  0.087842  0.119013  0.828757  ...  0.672356  0.633477  0.091810   \n",
       "7993  0.058850  0.195802  0.331647  ...  0.055077  0.407148  0.098213   \n",
       "\n",
       "           506       507       508       509       510       511      id  \n",
       "0     0.190971  3.211765  0.340266  0.552923  0.155349  0.457349  000190  \n",
       "1     0.299465  2.430354  0.070081  0.408074  0.040798  0.319569  000194  \n",
       "2     0.535053  1.876652  0.072166  0.499994  0.374152  0.979964  000667  \n",
       "3     0.037215  1.230592  0.068908  0.184140  0.181901  0.905914  001040  \n",
       "4     0.346600  2.577690  0.188910  0.500382  0.123293  0.147871  001686  \n",
       "...        ...       ...       ...       ...       ...       ...     ...  \n",
       "7989  0.083157  1.436705  0.035779  0.090603  0.087154  0.878640  149416  \n",
       "7990  0.089456  1.589537  0.057539  0.578900  0.031669  0.655243  149417  \n",
       "7991  0.027686  0.201245  0.149208  0.136870  1.800367  1.064121  149452  \n",
       "7992  0.047381  2.137580  0.119381  0.112560  0.056708  1.155992  149488  \n",
       "7993  1.139604  0.302552  0.024557  0.177658  0.374513  0.113956  149523  \n",
       "\n",
       "[7994 rows x 513 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = embedings[['id'] + list(np.arange(512))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings.to_csv('../data/csv/cnn_embedings_custom_resnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp_project",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
